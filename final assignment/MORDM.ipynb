{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multi-objective robust decision making (MORDM)\n",
    "\n",
    "\n",
    "This exercise demostrates the application of MORDM on the lake model, which was used in earlier exercises.\n",
    "\n",
    "MORDM has four main steps:\n",
    "\n",
    "(i)\t    **problem formulation** based on a systems analytical problem definition framework \n",
    "\n",
    "(ii)\t**searching** for candidate solutions that optimize multiple objectives by using multi-objective evolutionary algorithms \n",
    "\n",
    "(iii)\tgenerating an ensemble of scenarios to **explore** the effects of uncertainties \n",
    "\n",
    "(iv)\tusing **scenario discovery** to detect the vulnerabilities of candidate solutions and improving thecandidate solutions\n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Problem formulation\n",
    "### Lake Model\n",
    "\n",
    "Remember the lake problem used in the assignments in previous weeks. The lake problem is a hypothetical case where the inhabitants of a lake town decide on the amount of annual pollution they release into a lake. It the pollution in the lake passes a threshold, it will suffer irreversible eutrophication.\n",
    "\n",
    "The lake problem has 4 **outcome indicators**: \n",
    "   - **max_P**: maximum pollution over time, to be minimized\n",
    "   - **utility**: economic benefits obtained from polluting the lake, to be maximized\n",
    "   - **inertia**: the percentage of significant annual changes in the anthropogenic pollution rate, to be maximized\n",
    "   - **reliability**: the percentage of years where the pollution level is below the critical threshold, to be maximized\n",
    "    \n",
    "See the lake model exercise for the formulation of these outcome variables.\n",
    "\n",
    "The lake problem is characterized by both stochastic uncertainty and **deep uncertainty**. The stochastic uncertainty arises from the natural inflow. To reduce this stochastic uncertainty, multiple replications are performed and the average over the replication is taken. Deep uncertainty is presented by uncertainty about the mean $\\mu$ and standard deviation $sigma$ of the lognormal distribution characterizing the natural inflow, the natural removal rate of the lake $\\beta$, the natural recycling rate of the lake $q$, and the discount rate $\\delta$. The table below specifies the ranges for the deeply uncertain factors, as well as their best estimate or default values. \n",
    "\n",
    "|Parameter\t|Range\t        |Default value|\n",
    "|-----------|--------------:|------------:|\n",
    "|$\\mu$    \t|0.01 – 0.05\t|0.02         |\n",
    "|$\\sigma$\t|0.001 – 0.005 \t|0.0017       |\n",
    "|$b$      \t|0.1 – 0.45\t    |0.42         |\n",
    "|$q$\t    |2 – 4.5\t    |2            |\n",
    "|$\\delta$\t|0.93 – 0.99\t|0.98         |\n",
    "\n",
    "\n",
    "The lake problem in previous assignments had 100 decision **levers**, meaning that the decision makers independently decide on the amount of anthropogenic pollution at every time step (100). Then a 'policy' was a set of values for these 100 levers, which you composed by sampling from the range [0, 0.1].   \n",
    "\n",
    "In this exercise, we will use a more advanced way of deciding on the amout of anhtropogenic polution. We will use a **closed loop** version of the lake model, meaning that $a_t$ (anthropogenic pollution) is dependent on $X_t$ (the pollution level at time t). For instance, the rate of anthropogenic pollutions is lowered if the pollution level is approaching a critical threshold. Here, we use \"cubic radial basis functions\" following [Quinn et al. 2017](http://www.sciencedirect.com/science/article/pii/S1364815216302250) and formulate $a_t$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{t} =  min\\Bigg(max\\bigg(\\sum\\limits_{j=1}^{n} w_{j}\\left\\vert{\\frac{X_{t,i}-c_{j}}{r_{j}}}\\right\\vert^3, 0.01\\bigg), 0.1\\Bigg) \\\\\n",
    "    s.t. \\\\\n",
    "    -2 \\leq c_{j} \\leq 2 \\\\\n",
    "    0 \\leq r_{j} \\leq 2 \\\\ \n",
    "    0 \\leq w_{j} \\leq 1 \\\\\n",
    "    \\sum\\limits_{j=1}^{n} w_{j} = 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The parameters that define this function also define the pollution strategy over time. Hence, the decision **levers** are the five parameters of this functions, namely $c_1$, $c_2$, $r_1$, $r_2$ and $w_1$. ($w_2$ = 1 - $w_1$).\n",
    "\n",
    "Note:: i is index for the realization, given m realizations; j is the index for the radial basis function, given 2 radial basis functions. \n",
    "\n",
    "**To formulate this problem, do the following:**\n",
    "\n",
    "**1) Import the lake model function from dps_lake_model.py**\n",
    "\n",
    "**2) Create an ema_workbench interface for this problem, with corresponding uncertainties, levers and outcomes as specified above**\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:23:53.545415Z",
     "start_time": "2025-06-10T14:23:53.542200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from ema_workbench import (Model, RealParameter, ScalarOutcome)\n",
    "#\n",
    "# from dps_lake_model import lake_model\n",
    "#\n",
    "# model = Model('lakeproblem', function=lake_model)\n",
    "#\n",
    "# #specify uncertainties\n",
    "# model.uncertainties = [RealParameter('b', 0.1, 0.45),\n",
    "#                        RealParameter('q', 2.0, 4.5),\n",
    "#                        RealParameter('mean', 0.01, 0.05),\n",
    "#                        RealParameter('stdev', 0.001, 0.005),\n",
    "#                        RealParameter('delta', 0.93, 0.99)]\n",
    "#\n",
    "# # set levers\n",
    "# model.levers = [RealParameter(\"c1\", -2, 2),\n",
    "#                 RealParameter(\"c2\", -2, 2),\n",
    "#                 RealParameter(\"r1\", 0, 2),\n",
    "#                 RealParameter(\"r2\", 0, 2),\n",
    "#                 RealParameter(\"w1\", 0, 1)]\n",
    "#\n",
    "# #specify outcomes\n",
    "# # note how we need to explicitely indicate the direction\n",
    "# model.outcomes = [ScalarOutcome('max_P', kind=ScalarOutcome.MINIMIZE),\n",
    "#                   ScalarOutcome('utility', kind=ScalarOutcome.MAXIMIZE),\n",
    "#                   ScalarOutcome('inertia', kind=ScalarOutcome.MAXIMIZE),\n",
    "#                   ScalarOutcome('reliability', kind=ScalarOutcome.MAXIMIZE)]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:42:07.643956Z",
     "start_time": "2025-06-10T14:42:07.532311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from __future__ import (unicode_literals, print_function, absolute_import, division)\n",
    "\n",
    "from ema_workbench import (Model, MultiprocessingEvaluator, SequentialEvaluator, ScalarOutcome, Policy,\n",
    "                           RealParameter, CategoricalParameter, IntegerParameter, optimize, Scenario)\n",
    "\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress, HyperVolume\n",
    "from ema_workbench.util import ema_logging\n",
    "from ema_workbench.em_framework.samplers import sample_levers, sample_uncertainties, LHSSampler\n",
    "from ema_workbench.em_framework.salib_samplers import SobolSampler, MorrisSampler\n",
    "from ema_workbench import save_results, load_results\n",
    "from ema_workbench.analysis import parcoords\n",
    "from ema_workbench.analysis import prim\n",
    "\n",
    "#from problem_formulation import get_model_for_problem_formulation\n",
    "from problem_formulation2 import get_model_for_problem_formulation#, sum_over, sum_over_time\n",
    "\n",
    "\n",
    "# parameter definition for visualization libraries\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.set_style(\"whitegrid\")\n",
    "#name the specific dikes\n",
    "dikes = [f\"A.{i+1}\" for i in range(5)]\n",
    "\n",
    "#problem formulation selection\n",
    "#pf = 3\n",
    "problem_formulation_id =0\n",
    "direction = ScalarOutcome.MINIMIZE\n",
    "#Dependent on problem formulation used, outcomes are defined\n",
    "if problem_formulation_id == 0:\n",
    "    damage_variables = [f\"{dike}_Expected Annual Damage\" for dike in ['A.1', 'A.2', 'A.3']]\n",
    "    rfr_cost_variables = [\"RfR Total Costs\"]\n",
    "    dike_cost_variables = [f\"{dike}_Dike Investment Costs\" for dike in ['A.1', 'A.2', 'A.3']]\n",
    "\n",
    "    outcomes = [\n",
    "        ScalarOutcome(\n",
    "            \"Expected Annual Damage\",\n",
    "            variable_name=damage_variables,\n",
    "            kind=direction,\n",
    "        ),\n",
    "        ScalarOutcome(\n",
    "            \"RfR Total Costs\",\n",
    "            variable_name=rfr_cost_variables,\n",
    "            kind=direction,\n",
    "        ),\n",
    "        ScalarOutcome(\n",
    "            \"Dike Investment Costs\",\n",
    "            variable_name=dike_cost_variables,\n",
    "            kind=direction,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "# Uncertainties are defined uniformous, independent of problem formulation\n",
    "uncertainties = [IntegerParameter('A.0_ID flood wave shape', 0, 133),\n",
    "                   RealParameter('A.1_Bmax', 30,350),\n",
    "                   CategoricalParameter('A.1_Brate',(0.9,1.5,1000)),\n",
    "                   RealParameter('A.1_pfail',0,1),\n",
    "                   RealParameter('A.2_Bmax', 30,350),\n",
    "                   CategoricalParameter('A.2_Brate',(0.9,1.5,1000)),\n",
    "                   RealParameter('A.2_pfail',0,1),\n",
    "                   RealParameter('A.3_Bmax', 30,350),\n",
    "                   CategoricalParameter('A.3_Brate',(0.9,1.5,1000)),\n",
    "                   RealParameter('A.3_pfail',0,1),\n",
    "                   RealParameter('A.4_Bmax', 30,350),\n",
    "                   CategoricalParameter('A.4_Brate',(0.9,1.5,1000)),\n",
    "                   RealParameter('A.4_pfail',0,1),\n",
    "                   RealParameter('A.5_Bmax', 30,350),\n",
    "                   CategoricalParameter('A.5_Brate',(0.9,1.5,1000)),\n",
    "                   RealParameter('A.5_pfail',0,1),\n",
    "                   CategoricalParameter('discount rate 1',(1.5,2.5,3.5,4.5)),\n",
    "                   CategoricalParameter('discount rate 2',(1.5,2.5,3.5,4.5))]\n",
    "\n",
    "# Analogous to uncertainties model levers are defined\n",
    "levers = [IntegerParameter('A.1_DikeIncrease',0,10),\n",
    "                     IntegerParameter('A.2_DikeIncrease',0,10),\n",
    "                     IntegerParameter('A.3_DikeIncrease',0,10),\n",
    "                     IntegerParameter('A.4_DikeIncrease',0,10),\n",
    "                     IntegerParameter('A.5_DikeIncrease',0,2), #capped this variable to test if the model uses these specified levers\n",
    "                     IntegerParameter('1_RfR 0',0,1),\n",
    "                     IntegerParameter('2_RfR 0',0,1),\n",
    "                     IntegerParameter('3_RfR 0',0,1),\n",
    "                     IntegerParameter('4_RfR 0',0,1),\n",
    "                     IntegerParameter('0_RfR 0',0,1),\n",
    "                     IntegerParameter('EWS_DaysToThreat',0,4)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "    dike_model, function_planning_steps = get_model_for_problem_formulation(problem_formulation_id)\n",
    "\n",
    "    #setting up the reference scenario - needed for MORDM\n",
    "    reference_values = {'Bmax': 175, 'Brate': 1.5, 'pfail': 0.5,\n",
    "                        'discount rate': 3.5,\n",
    "                        'ID flood wave shape': 4}\n",
    "\n",
    "    # Definition of scenario-dictionary for later creation of policy class-instance for reference scenario\n",
    "    scen1 = {}\n",
    "\n",
    "    # reference scenario updated for all dike rings\n",
    "    for key in dike_model.uncertainties:\n",
    "        name_split = key.name.split('_')\n",
    "        if len(name_split) == 1:\n",
    "            if key.name in reference_values.keys():\n",
    "                scen1.update({key.name: reference_values[name]})\n",
    "        else:\n",
    "            scen1.update({key.name: reference_values[name_split[1]]})\n",
    "\n",
    "    #setting so that the reference scenario\n",
    "    ref_scenario = Scenario('reference', **scen1)\n",
    "\n",
    "    #setting up dike model uncertainties and potential ranges\n",
    "    dike_model.uncertainties = uncertainties\n",
    "\n",
    "    # set levers\n",
    "    dike_model.levers = levers\n",
    "\n",
    "    # set convergence metrics\n",
    "    convergence_metrics = [EpsilonProgress()]\n",
    "\n",
    "    # set nfe\n",
    "    #nfe = 100000\n",
    "    nfe=100"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:42:23.240944Z",
     "start_time": "2025-06-10T14:42:09.065597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#run optimization\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "        results, convergence = evaluator.optimize(nfe=nfe,\n",
    "                                                  searchover='levers',\n",
    "                                                  epsilons=[0.1,]*len(dike_model.outcomes),\n",
    "                                                  convergence=convergence_metrics,\n",
    "                                                  reference=ref_scenario\n",
    "                                                  )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started with 8 workers\n",
      "100%|████████████████████████████████████████| 100/100 [00:03<00:00, 25.27it/s]\n",
      "[MainProcess/INFO] optimization completed, found 4 solutions\n",
      "[MainProcess/INFO] terminating pool\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:42:23.264816Z",
     "start_time": "2025-06-10T14:42:23.256153Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0_RfR 0  0_RfR 1  0_RfR 2  1_RfR 0  1_RfR 1  1_RfR 2  2_RfR 0  2_RfR 1  \\\n",
       "0        1        1        1        1        0        0        0        0   \n",
       "1        1        1        0        1        1        0        1        1   \n",
       "2        0        0        1        0        0        0        0        1   \n",
       "3        0        0        0        1        1        0        1        0   \n",
       "\n",
       "   2_RfR 2  3_RfR 0  ...  A.5_DikeIncrease 1  A.5_DikeIncrease 2  \\\n",
       "0        1        1  ...                   2                   1   \n",
       "1        0        1  ...                   0                  10   \n",
       "2        1        1  ...                   3                   3   \n",
       "3        0        1  ...                   4                   1   \n",
       "\n",
       "   A.1_DikeIncrease  A.2_DikeIncrease  A.3_DikeIncrease  A.4_DikeIncrease  \\\n",
       "0                 4                 6                 2                 1   \n",
       "1                 4                 0                 8                10   \n",
       "2                 7                 4                 7                 8   \n",
       "3                 2                10                 2                 2   \n",
       "\n",
       "   A.5_DikeIncrease  Expected Annual Damage  RfR Total Costs  \\\n",
       "0                 0            0.000000e+00     8.796000e+08   \n",
       "1                 2            1.008150e+07     1.285900e+09   \n",
       "2                 2            0.000000e+00     3.884000e+08   \n",
       "3                 1            2.875878e+07     9.648000e+08   \n",
       "\n",
       "   Dike Investment Costs  \n",
       "0           3.630115e+08  \n",
       "1           2.805840e+08  \n",
       "2           3.729787e+08  \n",
       "3           2.851451e+08  \n",
       "\n",
       "[4 rows x 39 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_RfR 0</th>\n",
       "      <th>0_RfR 1</th>\n",
       "      <th>0_RfR 2</th>\n",
       "      <th>1_RfR 0</th>\n",
       "      <th>1_RfR 1</th>\n",
       "      <th>1_RfR 2</th>\n",
       "      <th>2_RfR 0</th>\n",
       "      <th>2_RfR 1</th>\n",
       "      <th>2_RfR 2</th>\n",
       "      <th>3_RfR 0</th>\n",
       "      <th>...</th>\n",
       "      <th>A.5_DikeIncrease 1</th>\n",
       "      <th>A.5_DikeIncrease 2</th>\n",
       "      <th>A.1_DikeIncrease</th>\n",
       "      <th>A.2_DikeIncrease</th>\n",
       "      <th>A.3_DikeIncrease</th>\n",
       "      <th>A.4_DikeIncrease</th>\n",
       "      <th>A.5_DikeIncrease</th>\n",
       "      <th>Expected Annual Damage</th>\n",
       "      <th>RfR Total Costs</th>\n",
       "      <th>Dike Investment Costs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.796000e+08</td>\n",
       "      <td>3.630115e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1.008150e+07</td>\n",
       "      <td>1.285900e+09</td>\n",
       "      <td>2.805840e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.884000e+08</td>\n",
       "      <td>3.729787e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.875878e+07</td>\n",
       "      <td>9.648000e+08</td>\n",
       "      <td>2.851451e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 39 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:42:29.555436Z",
     "start_time": "2025-06-10T14:42:29.468448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#done on another machine and imported here due to computational power\n",
    "#results = pd.read_csv(\"results/results_convergence_robustness_40000.csv\")\n",
    "#gives results across each of the dike rings for problem_formulation (3)\n",
    "for i in range(3):\n",
    "\n",
    "        data = results.loc[:, [f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               ]]\n",
    "\n",
    "        limits = parcoords.get_limits(data)\n",
    "        limits.loc[0, [f\"A.{i+1}_Expected Annual Damage\",\n",
    "                               f\"A.{i+1}_Expected Number of Deaths\",\n",
    "                               f\"A.{i+1}_Dike Investment Costs\",\n",
    "                               ]] = 0\n",
    "\n",
    "        paraxes = parcoords.ParallelAxes(limits)\n",
    "        paraxes.plot(data)\n",
    "\n",
    "        #paraxes.invert_axis('max_P')\n",
    "        plt.show()\n",
    "        #plt.savefig(f\"parcords_dikering_A.{i+1}.png\", dpi=300)"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['A.1_Dike Investment Costs', 'A.1_Expected Annual Damage',\\n       'A.1_Expected Number of Deaths'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#done on another machine and imported here due to computational power\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m#results = pd.read_csv(\"results/results_convergence_robustness_40000.csv\")\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m#gives results across each of the dike rings for problem_formulation (3)\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m3\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m         data = \u001B[43mresults\u001B[49m\u001B[43m.\u001B[49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mA.\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[43m+\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m_Dike Investment Costs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m                               \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mA.\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[43m+\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m_Expected Annual Damage\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m                               \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mA.\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[43m+\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m_Expected Number of Deaths\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m                               \u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     11\u001B[39m         limits = parcoords.get_limits(data)\n\u001B[32m     12\u001B[39m         limits.loc[\u001B[32m0\u001B[39m, [\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mA.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_Expected Annual Damage\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     13\u001B[39m                                \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mA.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_Expected Number of Deaths\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     14\u001B[39m                                \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mA.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_Dike Investment Costs\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     15\u001B[39m                                ]] = \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexing.py:1184\u001B[39m, in \u001B[36m_LocationIndexer.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1182\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._is_scalar_access(key):\n\u001B[32m   1183\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.obj._get_value(*key, takeable=\u001B[38;5;28mself\u001B[39m._takeable)\n\u001B[32m-> \u001B[39m\u001B[32m1184\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_getitem_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1185\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1186\u001B[39m     \u001B[38;5;66;03m# we by definition only have the 0th axis\u001B[39;00m\n\u001B[32m   1187\u001B[39m     axis = \u001B[38;5;28mself\u001B[39m.axis \u001B[38;5;129;01mor\u001B[39;00m \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexing.py:1377\u001B[39m, in \u001B[36m_LocIndexer._getitem_tuple\u001B[39m\u001B[34m(self, tup)\u001B[39m\n\u001B[32m   1374\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._multi_take_opportunity(tup):\n\u001B[32m   1375\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._multi_take(tup)\n\u001B[32m-> \u001B[39m\u001B[32m1377\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_getitem_tuple_same_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtup\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexing.py:1020\u001B[39m, in \u001B[36m_LocationIndexer._getitem_tuple_same_dim\u001B[39m\u001B[34m(self, tup)\u001B[39m\n\u001B[32m   1017\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m com.is_null_slice(key):\n\u001B[32m   1018\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1020\u001B[39m retval = \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mretval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1021\u001B[39m \u001B[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001B[39;00m\n\u001B[32m   1022\u001B[39m \u001B[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001B[39;00m\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m retval.ndim == \u001B[38;5;28mself\u001B[39m.ndim\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexing.py:1420\u001B[39m, in \u001B[36m_LocIndexer._getitem_axis\u001B[39m\u001B[34m(self, key, axis)\u001B[39m\n\u001B[32m   1417\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[33m\"\u001B[39m\u001B[33mndim\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key.ndim > \u001B[32m1\u001B[39m:\n\u001B[32m   1418\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mCannot index with multidimensional key\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1420\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_getitem_iterable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1422\u001B[39m \u001B[38;5;66;03m# nested tuple slicing\u001B[39;00m\n\u001B[32m   1423\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_nested_tuple(key, labels):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexing.py:1360\u001B[39m, in \u001B[36m_LocIndexer._getitem_iterable\u001B[39m\u001B[34m(self, key, axis)\u001B[39m\n\u001B[32m   1357\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_key(key, axis)\n\u001B[32m   1359\u001B[39m \u001B[38;5;66;03m# A collection of keys\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1360\u001B[39m keyarr, indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_listlike_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1361\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.obj._reindex_with_indexers(\n\u001B[32m   1362\u001B[39m     {axis: [keyarr, indexer]}, copy=\u001B[38;5;28;01mTrue\u001B[39;00m, allow_dups=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   1363\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexing.py:1558\u001B[39m, in \u001B[36m_LocIndexer._get_listlike_indexer\u001B[39m\u001B[34m(self, key, axis)\u001B[39m\n\u001B[32m   1555\u001B[39m ax = \u001B[38;5;28mself\u001B[39m.obj._get_axis(axis)\n\u001B[32m   1556\u001B[39m axis_name = \u001B[38;5;28mself\u001B[39m.obj._get_axis_name(axis)\n\u001B[32m-> \u001B[39m\u001B[32m1558\u001B[39m keyarr, indexer = \u001B[43max\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1560\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m keyarr, indexer\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6200\u001B[39m, in \u001B[36mIndex._get_indexer_strict\u001B[39m\u001B[34m(self, key, axis_name)\u001B[39m\n\u001B[32m   6197\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   6198\u001B[39m     keyarr, indexer, new_indexer = \u001B[38;5;28mself\u001B[39m._reindex_non_unique(keyarr)\n\u001B[32m-> \u001B[39m\u001B[32m6200\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6202\u001B[39m keyarr = \u001B[38;5;28mself\u001B[39m.take(indexer)\n\u001B[32m   6203\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[32m   6204\u001B[39m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6249\u001B[39m, in \u001B[36mIndex._raise_if_missing\u001B[39m\u001B[34m(self, key, indexer, axis_name)\u001B[39m\n\u001B[32m   6247\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m nmissing:\n\u001B[32m   6248\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m nmissing == \u001B[38;5;28mlen\u001B[39m(indexer):\n\u001B[32m-> \u001B[39m\u001B[32m6249\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m]\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   6251\u001B[39m     not_found = \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask.nonzero()[\u001B[32m0\u001B[39m]].unique())\n\u001B[32m   6252\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not in index\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyError\u001B[39m: \"None of [Index(['A.1_Dike Investment Costs', 'A.1_Expected Annual Damage',\\n       'A.1_Expected Number of Deaths'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:24:29.171672Z",
     "start_time": "2025-06-10T14:09:59.122584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ema_workbench import (Model, RealParameter, CategoricalParameter, IntegerParameter, ScalarOutcome)\n",
    "from dike_model_function import DikeNetwork\n",
    "import numpy as np\n",
    "\n",
    "# hulpmethode sum_over (die je outcomes gebruiken)\n",
    "# def sum_over(*args):\n",
    "#     numbers = []\n",
    "#     for entry in args:\n",
    "#         try:\n",
    "#             value = sum(entry)\n",
    "#         except TypeError:\n",
    "#             value = entry\n",
    "#         numbers.append(value)\n",
    "#     return sum(numbers)\n",
    "\n",
    "# model\n",
    "model = Model('dikeproblem', function=DikeNetwork())\n",
    "\n",
    "# specify uncertainties\n",
    "model.uncertainties = [\n",
    "    # uncertainties per dike\n",
    "    RealParameter('A.1_Bmax', 30, 350),\n",
    "    RealParameter('A.1_pfail', 0, 1),\n",
    "    CategoricalParameter('A.1_Brate', [1.0, 1.5, 10]),\n",
    "\n",
    "    RealParameter('A.2_Bmax', 30, 350),\n",
    "    RealParameter('A.2_pfail', 0, 1),\n",
    "    CategoricalParameter('A.2_Brate', [1.0, 1.5, 10]),\n",
    "\n",
    "    RealParameter('A.3_Bmax', 30, 350),\n",
    "    RealParameter('A.3_pfail', 0, 1),\n",
    "    CategoricalParameter('A.3_Brate', [1.0, 1.5, 10]),\n",
    "\n",
    "    # discount rate per planning step (planning_steps = [0,1,2,3])\n",
    "    CategoricalParameter('discount rate 0', [1.5, 2.5, 3.5, 4.5]),\n",
    "    CategoricalParameter('discount rate 1', [1.5, 2.5, 3.5, 4.5]),\n",
    "    CategoricalParameter('discount rate 2', [1.5, 2.5, 3.5, 4.5]),\n",
    "    CategoricalParameter('discount rate 3', [1.5, 2.5, 3.5, 4.5]),\n",
    "\n",
    "    # flood wave shape\n",
    "    IntegerParameter('A.0_ID flood wave shape', 0, 132)\n",
    "]\n",
    "\n",
    "# set levers\n",
    "model.levers = [\n",
    "    # DikeIncrease per dike en per planning step\n",
    "    IntegerParameter('A.1_DikeIncrease 0', 0, 10),\n",
    "    IntegerParameter('A.1_DikeIncrease 1', 0, 10),\n",
    "    IntegerParameter('A.1_DikeIncrease 2', 0, 10),\n",
    "\n",
    "    IntegerParameter('A.2_DikeIncrease 0', 0, 10),\n",
    "    IntegerParameter('A.2_DikeIncrease 1', 0, 10),\n",
    "    IntegerParameter('A.2_DikeIncrease 2', 0, 10),\n",
    "\n",
    "    IntegerParameter('A.3_DikeIncrease 0', 0, 10),\n",
    "    IntegerParameter('A.3_DikeIncrease 1', 0, 10),\n",
    "    IntegerParameter('A.3_DikeIncrease 2', 0, 10),\n",
    "\n",
    "    # RfR per project en per planning step (5 projecten: 0 t/m 4)\n",
    "    IntegerParameter('0_RfR 0', 0, 1),\n",
    "    IntegerParameter('0_RfR 1', 0, 1),\n",
    "    IntegerParameter('0_RfR 2', 0, 1),\n",
    "\n",
    "    IntegerParameter('1_RfR 0', 0, 1),\n",
    "    IntegerParameter('1_RfR 1', 0, 1),\n",
    "    IntegerParameter('1_RfR 2', 0, 1),\n",
    "\n",
    "    IntegerParameter('2_RfR 0', 0, 1),\n",
    "    IntegerParameter('2_RfR 1', 0, 1),\n",
    "    IntegerParameter('2_RfR 2', 0, 1),\n",
    "\n",
    "    IntegerParameter('3_RfR 0', 0, 1),\n",
    "    IntegerParameter('3_RfR 1', 0, 1),\n",
    "    IntegerParameter('3_RfR 2', 0, 1),\n",
    "\n",
    "    IntegerParameter('4_RfR 0', 0, 1),\n",
    "    IntegerParameter('4_RfR 1', 0, 1),\n",
    "    IntegerParameter('4_RfR 2', 0, 1),\n",
    "\n",
    "    # Early Warning System\n",
    "    IntegerParameter('EWS_DaysToThreat', 0, 4)\n",
    "]\n",
    "\n",
    "model.outcomes = [\n",
    "    ScalarOutcome('RfR Total Costs', kind=ScalarOutcome.MINIMIZE),\n",
    "    ScalarOutcome('Dike Investment Costs', kind=ScalarOutcome.MINIMIZE),\n",
    "    ScalarOutcome('Expected Annual Damage', kind=ScalarOutcome.MINIMIZE)\n",
    " ]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:24:29.182990Z",
     "start_time": "2025-06-10T14:10:00.747241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from ema_workbench import (\n",
    "#     Model,\n",
    "#     Policy,\n",
    "#     ema_logging,\n",
    "#     SequentialEvaluator,\n",
    "#     MultiprocessingEvaluator,\n",
    "# )\n",
    "# from dike_model_function import DikeNetwork  # @UnresolvedImport\n",
    "# from problem_formulation2 import get_model_for_problem_formulation, sum_over, sum_over_time\n",
    "#\n",
    "# ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "#\n",
    "# # 0: is our problem formulation (Veluwe); 1: different problem formulation with 2 kinds of costs together for Veluwe.\n",
    "# # 2: problem formulation Zutphen, 3: problem formulation Doesburg & Cortenoever; 4: problem formulation Overijssel\n",
    "# dike_model, planning_steps = get_model_for_problem_formulation(0)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Searching for candidate solutions\n",
    "\n",
    "In the second step of MORDM, candidate strategies are identified which are pareto optimal conditional on a reference scenario. These candiate strategies are identified through search with multi-objective evolutionary algorithms, that iteratively evaluate a large number of alternatives on multiple objectives until they find the best candidates. For instance, when we optimize the lake model levers, the lake model function will be called for each candidate evaluation, and the corresponding four objective values will be generated. \n",
    "\n",
    "Take the model interface developed in the previous step and use the optimization functionality of the workbench to identify the pareto approximate set of solutions. Try the following:\n",
    "* change the epsilon values between 0.01 and 0.1, what changes, why?\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:24:29.184560Z",
     "start_time": "2025-06-10T14:10:00.778978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ema_workbench import Model\n",
    "\n",
    "class PatchedDikeModel(Model):\n",
    "    def run_experiment(self, experiment):\n",
    "        experiment = experiment.copy()\n",
    "\n",
    "        # Provide fallback keys in case the model incorrectly looks for unprefixed ones\n",
    "        for dike in [\"A.1\", \"A.2\", \"A.3\"]:\n",
    "            for step in [0, 1, 2]:\n",
    "                full_key = f\"{dike}_DikeIncrease {step}\"\n",
    "                wrong_key = f\"DikeIncrease {step}\"\n",
    "                # Only patch if not already set (avoid overwriting correct ones)\n",
    "                if full_key in experiment and wrong_key not in experiment:\n",
    "                    experiment[wrong_key] = experiment[full_key]\n",
    "\n",
    "        return super().run_experiment(experiment)\n",
    "\n",
    "patched_model = PatchedDikeModel(name=model.name, function=model.function)\n",
    "patched_model.uncertainties = model.uncertainties\n",
    "patched_model.levers = model.levers\n",
    "patched_model.outcomes = model.outcomes"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:24:29.185376Z",
     "start_time": "2025-06-10T14:10:00.801273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ema_workbench import SequentialEvaluator\n",
    "\n",
    "with SequentialEvaluator(patched_model) as evaluator:\n",
    "    results = evaluator.optimize(nfe=500, searchover='levers',  # lower nfe for speed\n",
    "                                 epsilons=[0.1] * len(patched_model.outcomes))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/500 [00:00<?, ?it/s]not enough values to unpack (expected 2, got 1)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/evalievanoijen/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/experiment_runner.py\", line 92, in run_experiment\n",
      "    model.run_model(scenario, policy)\n",
      "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/evalievanoijen/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/util/ema_logging.py\", line 153, in wrapper\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/Users/evalievanoijen/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/model.py\", line 347, in run_model\n",
      "    outputs = self.run_experiment(experiment)\n",
      "  File \"/var/folders/cq/_rbxzkq17533m5qdt1c__dyw0000gn/T/ipykernel_44112/2576972648.py\", line 16, in run_experiment\n",
      "    return super().run_experiment(experiment)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"/Users/evalievanoijen/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/util/ema_logging.py\", line 153, in wrapper\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/Users/evalievanoijen/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/model.py\", line 400, in run_experiment\n",
      "    model_output = self.function(**experiment)\n",
      "  File \"/Users/evalievanoijen/PycharmProjects/group25_mbdm/final assignment/dike_model_function.py\", line 141, in __call__\n",
      "    string1, string2 = item.split(\"_\")\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "ename": "EMAError",
     "evalue": "Exception in run_model\nCaused by: ValueError: not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/experiment_runner.py:92\u001B[39m, in \u001B[36mExperimentRunner.run_experiment\u001B[39m\u001B[34m(self, experiment)\u001B[39m\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m92\u001B[39m     \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscenario\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m CaseError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/util/ema_logging.py:153\u001B[39m, in \u001B[36mmethod_logger.<locals>.real_decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    154\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcompleted calling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/model.py:347\u001B[39m, in \u001B[36mSingleReplication.run_model\u001B[39m\u001B[34m(self, scenario, policy)\u001B[39m\n\u001B[32m    345\u001B[39m experiment = ExperimentReplication(scenario, \u001B[38;5;28mself\u001B[39m.policy, constants)\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[38;5;28mself\u001B[39m.outcomes_output = outputs\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mPatchedDikeModel.run_experiment\u001B[39m\u001B[34m(self, experiment)\u001B[39m\n\u001B[32m     14\u001B[39m             experiment[wrong_key] = experiment[full_key]\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiment\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/util/ema_logging.py:153\u001B[39m, in \u001B[36mmethod_logger.<locals>.real_decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    154\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcompleted calling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/model.py:400\u001B[39m, in \u001B[36mBaseModel.run_experiment\u001B[39m\u001B[34m(self, experiment)\u001B[39m\n\u001B[32m    393\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Method for running an instantiated model structure.\u001B[39;00m\n\u001B[32m    394\u001B[39m \n\u001B[32m    395\u001B[39m \u001B[33;03mParameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    398\u001B[39m \n\u001B[32m    399\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m400\u001B[39m model_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mexperiment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[38;5;66;03m# TODO: might it be possible to somehow abstract this\u001B[39;00m\n\u001B[32m    403\u001B[39m \u001B[38;5;66;03m# perhaps expose a get_data on modelInterface?\u001B[39;00m\n\u001B[32m    404\u001B[39m \u001B[38;5;66;03m# different connectors can than implement only this\u001B[39;00m\n\u001B[32m    405\u001B[39m \u001B[38;5;66;03m# get method\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/final assignment/dike_model_function.py:141\u001B[39m, in \u001B[36mDikeNetwork.__call__\u001B[39m\u001B[34m(self, timestep, **kwargs)\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[38;5;66;03m# the rest of the times you always get a string like {}_{}:\u001B[39;00m\n\u001B[32m    140\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m     string1, string2 = item.split(\u001B[33m\"\u001B[39m\u001B[33m_\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    143\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mRfR\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m string2:\n\u001B[32m    144\u001B[39m         \u001B[38;5;66;03m# string1: projectID\u001B[39;00m\n\u001B[32m    145\u001B[39m         \u001B[38;5;66;03m# string2: rfr #step\u001B[39;00m\n\u001B[32m    146\u001B[39m         \u001B[38;5;66;03m# Note: kwargs[item] in this case can be either 0\u001B[39;00m\n\u001B[32m    147\u001B[39m         \u001B[38;5;66;03m# (no project) or 1 (yes project)\u001B[39;00m\n",
      "\u001B[31mValueError\u001B[39m: not enough values to unpack (expected 2, got 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mEMAError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mema_workbench\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SequentialEvaluator\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SequentialEvaluator(patched_model) \u001B[38;5;28;01mas\u001B[39;00m evaluator:\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     results = \u001B[43mevaluator\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnfe\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msearchover\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlevers\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# lower nfe for speed\u001B[39;49;00m\n\u001B[32m      5\u001B[39m \u001B[43m                                 \u001B[49m\u001B[43mepsilons\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpatched_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43moutcomes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/evaluators.py:228\u001B[39m, in \u001B[36mBaseEvaluator.optimize\u001B[39m\u001B[34m(self, algorithm, nfe, searchover, reference, constraints, convergence_freq, logging_freq, variator, **kwargs)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptimize\u001B[39m(\n\u001B[32m    211\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    212\u001B[39m     algorithm=EpsNSGAII,\n\u001B[32m   (...)\u001B[39m\u001B[32m    220\u001B[39m     **kwargs,\n\u001B[32m    221\u001B[39m ):\n\u001B[32m    222\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"convenience method for outcome optimization.\u001B[39;00m\n\u001B[32m    223\u001B[39m \n\u001B[32m    224\u001B[39m \u001B[33;03m    is forwarded to :func:optimize, with evaluator and models\u001B[39;00m\n\u001B[32m    225\u001B[39m \u001B[33;03m    arguments added in.\u001B[39;00m\n\u001B[32m    226\u001B[39m \n\u001B[32m    227\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m228\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_msis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    230\u001B[39m \u001B[43m        \u001B[49m\u001B[43malgorithm\u001B[49m\u001B[43m=\u001B[49m\u001B[43malgorithm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    231\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnfe\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnfe\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    232\u001B[39m \u001B[43m        \u001B[49m\u001B[43msearchover\u001B[49m\u001B[43m=\u001B[49m\u001B[43msearchover\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[43m        \u001B[49m\u001B[43mevaluator\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    234\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreference\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    235\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconstraints\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconstraints\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    236\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvergence_freq\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvergence_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogging_freq\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogging_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvariator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvariator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    240\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/evaluators.py:576\u001B[39m, in \u001B[36moptimize\u001B[39m\u001B[34m(models, algorithm, nfe, searchover, evaluator, reference, convergence, constraints, convergence_freq, logging_freq, variator, **kwargs)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m evaluator:\n\u001B[32m    574\u001B[39m     evaluator = SequentialEvaluator(models)\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    577\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproblem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevaluator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    579\u001B[39m \u001B[43m    \u001B[49m\u001B[43malgorithm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    580\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconvergence\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    581\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnfe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    582\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconvergence_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    583\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    584\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvariator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvariator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    585\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    586\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/optimization.py:1101\u001B[39m, in \u001B[36m_optimize\u001B[39m\u001B[34m(problem, evaluator, algorithm, convergence, nfe, convergence_freq, logging_freq, variator, **kwargs)\u001B[39m\n\u001B[32m   1098\u001B[39m evaluator.callback = callback\n\u001B[32m   1100\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m temporary_filter(name=[callbacks.\u001B[34m__name__\u001B[39m, evaluators.\u001B[34m__name__\u001B[39m], level=INFO):\n\u001B[32m-> \u001B[39m\u001B[32m1101\u001B[39m     \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnfe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1103\u001B[39m convergence(optimizer, force=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m   1105\u001B[39m \u001B[38;5;66;03m# convergence.pbar.__exit__(None, None, None)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/platypus/core.py:741\u001B[39m, in \u001B[36mAlgorithm.run\u001B[39m\u001B[34m(self, condition, callback)\u001B[39m\n\u001B[32m    738\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m extension \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._extensions:\n\u001B[32m    739\u001B[39m     extension.pre_step(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m741\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    743\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m extension \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._extensions:\n\u001B[32m    744\u001B[39m     extension.post_step(\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/platypus/algorithms.py:304\u001B[39m, in \u001B[36mNSGAII.step\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    303\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.nfe == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m304\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minitialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    305\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    306\u001B[39m         \u001B[38;5;28mself\u001B[39m.iterate()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/platypus/algorithms.py:314\u001B[39m, in \u001B[36mNSGAII.initialize\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    313\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minitialize\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m314\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43minitialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    316\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.archive \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    317\u001B[39m         \u001B[38;5;28mself\u001B[39m.archive += \u001B[38;5;28mself\u001B[39m.population\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/platypus/algorithms.py:106\u001B[39m, in \u001B[36mAbstractGeneticAlgorithm.initialize\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    104\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Initializes the algorithm.\"\"\"\u001B[39;00m\n\u001B[32m    105\u001B[39m \u001B[38;5;28mself\u001B[39m.population = [\u001B[38;5;28mself\u001B[39m.generator.generate(\u001B[38;5;28mself\u001B[39m.problem) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m.population_size)]\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mevaluate_all\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpopulation\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/platypus/core.py:702\u001B[39m, in \u001B[36mAlgorithm.evaluate_all\u001B[39m\u001B[34m(self, solutions)\u001B[39m\n\u001B[32m    699\u001B[39m unevaluated = [s \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m solutions \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s.evaluated]\n\u001B[32m    701\u001B[39m jobs = [EvaluateSolution(s) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m unevaluated]\n\u001B[32m--> \u001B[39m\u001B[32m702\u001B[39m results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mevaluator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevaluate_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    704\u001B[39m \u001B[38;5;66;03m# if needed, update the original solution with the results\u001B[39;00m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, result \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(results):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/evaluators.py:153\u001B[39m, in \u001B[36mBaseEvaluator.evaluate_all\u001B[39m\u001B[34m(self, jobs, **kwargs)\u001B[39m\n\u001B[32m    150\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m()\n\u001B[32m    152\u001B[39m \u001B[38;5;66;03m# overwrite the default 10 progress reports  with 5 reports\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m callback = \u001B[43mperform_experiments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    154\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_msis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    155\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevaluator\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    156\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreporting_frequency\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreporting_frequency\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscenarios\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscenarios\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpolicies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_callback\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlog_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    163\u001B[39m experiments, outcomes = callback.get_results()\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m searchover \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33mlevers\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33muncertainties\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/evaluators.py:406\u001B[39m, in \u001B[36mperform_experiments\u001B[39m\u001B[34m(models, scenarios, policies, evaluator, reporting_interval, reporting_frequency, uncertainty_union, lever_union, outcome_union, uncertainty_sampling, lever_sampling, callback, return_callback, combine, log_progress, **kwargs)\u001B[39m\n\u001B[32m    403\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m evaluator:\n\u001B[32m    404\u001B[39m     evaluator = SequentialEvaluator(models)\n\u001B[32m--> \u001B[39m\u001B[32m406\u001B[39m \u001B[43mevaluator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevaluate_experiments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscenarios\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcombine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcombine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m callback.i != nr_of_exp:\n\u001B[32m    409\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EMAError(\n\u001B[32m    410\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSome fatal error has occurred while running the experiments, not all runs have completed. Expected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnr_of_exp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcallback.i\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    411\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/evaluators.py:291\u001B[39m, in \u001B[36mSequentialEvaluator.evaluate_experiments\u001B[39m\u001B[34m(self, scenarios, policies, callback, combine)\u001B[39m\n\u001B[32m    288\u001B[39m runner = ExperimentRunner(models)\n\u001B[32m    290\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m experiment \u001B[38;5;129;01min\u001B[39;00m ex_gen:\n\u001B[32m--> \u001B[39m\u001B[32m291\u001B[39m     outcomes = \u001B[43mrunner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    292\u001B[39m     callback(experiment, outcomes)\n\u001B[32m    293\u001B[39m runner.cleanup()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/util/ema_logging.py:153\u001B[39m, in \u001B[36mmethod_logger.<locals>.real_decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args, **kwargs):\n\u001B[32m    150\u001B[39m     \u001B[38;5;66;03m# hack, because log is applied to methods, we can get\u001B[39;00m\n\u001B[32m    151\u001B[39m     \u001B[38;5;66;03m# object instance as first arguments in args\u001B[39;00m\n\u001B[32m    152\u001B[39m     logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m     res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    154\u001B[39m     logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcompleted calling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    155\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/group25_mbdm/venv/lib/python3.13/site-packages/ema_workbench/em_framework/experiment_runner.py:108\u001B[39m, in \u001B[36mExperimentRunner.run_experiment\u001B[39m\u001B[34m(self, experiment)\u001B[39m\n\u001B[32m    102\u001B[39m     \u001B[38;5;66;03m#             exception = traceback.print_exc()\u001B[39;00m\n\u001B[32m    103\u001B[39m     \u001B[38;5;66;03m#             if exception:\u001B[39;00m\n\u001B[32m    104\u001B[39m     \u001B[38;5;66;03m#                 sys.stderr.write(exception)\u001B[39;00m\n\u001B[32m    105\u001B[39m     \u001B[38;5;66;03m#                 sys.stderr.write(\"\\n\")\u001B[39;00m\n\u001B[32m    107\u001B[39m     errortype = \u001B[38;5;28mtype\u001B[39m(e).\u001B[34m__name__\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EMAError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mException in run_model\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mCaused by: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merrortype\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    110\u001B[39m outcomes = model.outcomes_output\n\u001B[32m    111\u001B[39m model.reset_model()\n",
      "\u001B[31mEMAError\u001B[39m: Exception in run_model\nCaused by: ValueError: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# epsilons= [.005, .005, .005, .005, .005]\n",
    "# convergence = [EpsilonProgress()]\n",
    "#\n",
    "# # convergence = [HyperVolume(minimum=[0,0,0,0,0], maximum=[1e+08, 1e+09, 1e+10, 1000, .1]),\n",
    "# #                EpsilonProgress()]\n",
    "# with MultiprocessingEvaluator(model) as evaluator:\n",
    "#     output = evaluator.optimize(nfe=nfe, searchover='levers', reference = scenarios[case],\n",
    "#                                               epsilons=epsilons,\n",
    "#                                               convergence=convergence)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from ema_workbench import MultiprocessingEvaluator, ema_logging\n",
    "#\n",
    "# ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "#\n",
    "# with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "#     results = evaluator.optimize(nfe=250, searchover=\"levers\", epsilons=[0.1] * len(dike_model.outcomes))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results2 = evaluator.optimize(nfe=5e3, searchover='levers',\n",
    "                                 epsilons=[0.01,]*len(dike_model.outcomes))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = results1.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = results2.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above parallel coordinate plots are consistent with our expectation. If we use a lower value for epsilon, we have many more solutions in the pareto approximate set.\n",
    "\n",
    "* change the number of function evaluations from 1000 to 10.000 (this requires using multiprocessing unless you are very patient). What is the difference? You can use  convergence as explained in assignment 7 for this\n",
    "\n",
    "Calculating hypervolume can be tricky. It requires specifying the minimum and maximum values that can occur. That is, we basically have to describe the hyperbox within which hypervolume needs to be calculated. There are various ways we can ensure this. For example we take the values quite broad to be certain. We can also add constraints to the outcome space. A common practice, however, is to use the minima and maxima from a reference set. This reference set can come from various sources. In some cases we have analytical solutions which we can use for the reference set. However, in many real world problems such analytical solutions don't exist. So, instead we use as a reference set, the set of best known solutions (*i.e.*, the final archive, or the set of archives merged across multiple seeds). This is also what I do below.\n",
    "\n",
    "Calculating hypervolume is computationally expensive. Therefore, it might sometimes be better to simply log the state of the achive at each generation, and only calculate hypervolume afterwards. See the [ArchiveLogger](http://emaworkbench.readthedocs.io/en/latest/ema_documentation/em_framework/optimization.html#ema_workbench.em_framework.optimization.ArchiveLogger) which you can use just like other convergence metrics for details\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "random.seed(20)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench.em_framework.optimization import (ArchiveLogger,\n",
    "                                                     EpsilonProgress)\n",
    "\n",
    "convergence_metrics = [\n",
    "    ArchiveLogger(\n",
    "        \"./archives\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"assignment_9_1.tar.gz\",\n",
    "    ),\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=1e3, searchover='levers',\n",
    "                                 convergence=convergence_metrics,\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "archives = ArchiveLogger.load_archives(\"./archives/assignment_9_1.tar.gz\")"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench import HypervolumeMetric\n",
    "from ema_workbench.em_framework.optimization import to_problem\n",
    "\n",
    "reference_set = results\n",
    "problem = to_problem(model, searchover=\"levers\")\n",
    "\n",
    "hv = HypervolumeMetric(reference_set, problem)\n",
    "\n",
    "hypervolume = [(nfe, hv.calculate(archive)) for nfe, archive in archives.items()]\n",
    "hypervolume.sort(key=lambda x:x[0])\n",
    "hypervolume = np.asarray(hypervolume)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel(r'$\\epsilon$-progress')\n",
    "ax2.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "convergence_metrics = [\n",
    "    ArchiveLogger(\n",
    "        \"./archives\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"assignment_9_2.tar.gz\",\n",
    "    ),\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=1e4, searchover='levers',\n",
    "                                 convergence=convergence_metrics,\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "archives = ArchiveLogger.load_archives(\"./archives/assignment_9_2.tar.gz\")\n",
    "\n",
    "reference_set = results\n",
    "problem = to_problem(model, searchover=\"levers\")\n",
    "\n",
    "hv = HypervolumeMetric(reference_set, problem)\n",
    "\n",
    "hypervolume = [(nfe, hv.calculate(archive)) for nfe, archive in archives.items()]\n",
    "hypervolume.sort(key=lambda x:x[0])\n",
    "hypervolume = np.asarray(hypervolume)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel(r'$\\epsilon$-progress')\n",
    "ax2.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, the optimization has converged better in the second case with the higher number of function evaluations. Note also how in this particular case, search seems to stagnate before picking up again close to 10,000. So, to be certain, it would be good practice to use an even larger number of nfe (I would start wtih at least 50k, preferably even 250k), as shown below.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "convergence_metrics = [\n",
    "    ArchiveLogger(\n",
    "        \"./archives\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"assignment_9_3.tar.gz\",\n",
    "    ),\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=1e5, searchover='levers',\n",
    "                                 convergence=convergence_metrics,\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "archives = ArchiveLogger.load_archives(\"./archives/assignment_9_3.tar.gz\")\n",
    "\n",
    "reference_set = results\n",
    "problem = to_problem(model, searchover=\"levers\")\n",
    "\n",
    "hv = HypervolumeMetric(reference_set, problem)\n",
    "\n",
    "hypervolume = [(nfe, hv.calculate(archive)) for nfe, archive in archives.items()]\n",
    "hypervolume.sort(key=lambda x:x[0])\n",
    "hypervolume = np.asarray(hypervolume)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel(r'$\\epsilon$-progress')\n",
    "ax2.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "ax2.set_ylim(ymin=0)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can more clearly see here, the early stalled search we saw was an artifact and we need at least 100k before we can have some confidence in convergence."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**plot the tradeoffs you have found using a parallel axis plot**\n",
    "\n",
    "We can visualize these tradeoffs on a **parallel axis plots**. In these plots, each dimension is shown as a vertical axis. Each solution is represented by a line on this plot, which crosses the objective axes at the corresponsing value. You can use the [parcoords functionality](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/analysis/parcoords.html) for this that comes with the ema_workbench. Ensure that the direction of desirability is the same for the four objectives.|\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = results.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**What does this plot tell us about the tradeoffs and conflicting objectives?**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Max_P is negatively correlated with utility and intertia, and positevely with reliability. if we look at inertia, there are two solutions which sacrifice intertia for higher scores on reliability. These might reflect a different correlation structure, or be evidence of incomplete convergence. \n",
    "\n",
    "\n",
    "## Step 3: Re-evaluate candidate solutions under uncertainty\n",
    "\n",
    "We now have a large number of candidate solutions (policies), we can re-evaluate them over the various deeply uncertain factors to assess their robustness against uncertainties.\n",
    "\n",
    "For this robustness evaluation, we need to explore the scenarios for each solution. It means that, if we would like to run for instance 1000 scenarios for each solution, we might have to execute a very large number of runs.\n",
    "\n",
    "Here, to simplify the case, let's suppose that decision makers have a hard constrain on *reliability*. No solution with less than 90% reliability is acceptable for them. Therefore, we can reduce the size of the solution set according to this constraint. \n",
    "\n",
    "**Apply this constraint of reliability on the results, and create a new dataframe named new_results**\n",
    "\n",
    "**From new_results, which is the reduced dataframe of candidate solutions, make a list of policies in a format that can be inputed to the *perform_experiments* function of the EMA workbench.**\n",
    "\n",
    "*hint: you need to transform each policy to a dict, and then use this dict as input for the Policy class that comes with the workbench*\n",
    "\n",
    "There are various ways to do it. One way is to use logical indexing. Basically, create a boolean vector that indicates for each row if the constraint is met or not. Next, we can use this as an index on the dataframe to get only the rows for which the index is true.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logical = results.reliability > 0.9\n",
    "np.sum(logical)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We only have one solution that meets the constraints. Given that below we need to calculate regret (which requires more than one solution), let's try a slighly less stringent constraint."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logical = results.reliability > 0.75\n",
    "np.sum(logical)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2 solutions is better, so we can proceed."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results[logical]"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "policies = results[logical]\n",
    "policies = policies.drop([o.name for o in model.outcomes], axis=1)\n",
    "policies"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench import Policy\n",
    "\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Perform 1000 scenarios for each of the policy options. Depending on how many solutions are left after implementing the constraint, consider using multiprocessing or ipyparallel to speed up calculations.**\n",
    "\n",
    "If you want to use ipyparallel, don't forget to start ipcluster."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_scenarios = 1000\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can now evaluate the **robustness** of each of the policy options based on these scenario results. We can calculate the robustness of a policy option in terms of its performance on an outcome indicator across the 1000 scenarios. In other words, we can identify how robust a policy is in terms of each outcome indicator, and investigate the robustness tradeoffs.  \n",
    "\n",
    "There are multiple metrics to quantify robustness. On of them is the *signal to noise ratio*, which is simply the mean of a dataset divided by its standard deviation. For instance, for an outcome indicator to be maximized, we prefer a high average value across the scenarios, and a low standard deviation, implying a narrow range of uncertainty about the outcomes. Therefore, we want to maximize the signal-to-noise ratio. For an outcome indicator to be minimized, a lower mean and a lower standard deviation is preferred. Therefore the formulation should be different.\n",
    "\n",
    "**Write a function to calculate the signal-to-noise ratio for both kinds of outcome indicators. Calculate the signal-to-noise ratios for each outcome and each policy option. Plot the tradeoffs on a parallel plot. Which solutions look like a good compromise policy?**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def s_to_n(data, direction):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    if std==0:\n",
    "        std = 1\n",
    "    \n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std\n",
    "    "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are various ways in which we could calculate the signal to noise ratio. Here, I choose to iterate over the policy first. Next, I iterate over the outcomes. For each outcome, I only retrieve the results associated with the current policy. Next, I can calculate the signal to noise ratio. \n",
    "\n",
    "I am reusing the direction that we already specified for each outcome of interest, to avoid duplicating code. Note that I enabled this already with how I defined the s_to_n function above.\n",
    "\n",
    "To make visualization easy, I transform all my results into a dataframe at the end."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    \n",
    "    logical = experiments['policy']==policy\n",
    "    \n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The ideal solution would be a a high Signal to Noise score for reliability, inertia, and utility, and a low score for max_P. Such a solution clearly does not exist. The best solution for max_P and reliability score low on inertia and utility. No compromise solution can thus be found in this case. \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another robustness metric is **maximum regret**, calculated again for each policy and for each outcome indicator. *Regret* is defined for each policy under each scenario, as the difference between the performance of the policy in a specific scenario and the berformance of a no-regret (i.e. best possible result in that scenario) or reference policy. The *maximum regret*  is then the maximum of such regret values across all scenarios. We of course favor policy options with low *maximum regret* values. \n",
    "\n",
    "**Write a function to calculate the maximum regret for both kinds of outcome indicators. Calculate the maximum regret values for each outcome and each policy option. Plot the tradeoffs on a parallel plot. Which solutions look like a good compromise policy?**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_regret(data, best):\n",
    "    return np.abs(best-data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Regret is the performance difference between the best possible outcome in a scenario across policies, the the observed outcome for a given policy. We have in this case both minimization and maximization. Best means the lowest in case of mimimization, and heighest in case of maximization. To avoid having to explicitly account for this in how we calculate the difference, we can simply take the absolute value of the difference. In this case, max_P will return negative regret values for `best-data`, so by taking the absolute value, we fix this\n",
    "\n",
    "The next part of the code is probably the most tricky part. We need to find the best possible outcome for each scenario. We could do this by iterating over the scenario_id column in the experiment array. But we can also use pandas instead as done below. Wat we do is the following:\n",
    "1. we create a dataframe with the outcome, the name of the policy and the scenario. This is a so called long-form representation of the data\n",
    "2. We want to have the results for each policy side by side so we can take the max, or min accross the column. The pivot method on the DataFrame does this for us\n",
    "3. We take the maximum or minimum accross the row."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_regret = {}\n",
    "max_regret = {}\n",
    "for outcome in model.outcomes:\n",
    "    policy_column = experiments['policy']\n",
    "    \n",
    "    # create a DataFrame with all the relevent information\n",
    "    # i.e., policy, scenario_id, and scores\n",
    "    data = pd.DataFrame({outcome.name: outcomes[outcome.name], \n",
    "                         \"policy\":experiments['policy'],\n",
    "                         \"scenario\":experiments['scenario']})\n",
    "    \n",
    "    # reorient the data by indexing with policy and scenario id\n",
    "    data = data.pivot(index='scenario', columns='policy')\n",
    "    \n",
    "    # flatten the resulting hierarchical index resulting from \n",
    "    # pivoting, (might be a nicer solution possible)\n",
    "    data.columns = data.columns.get_level_values(1)\n",
    "    \n",
    "    # we need to control the broadcasting. \n",
    "    # max returns a 1d vector across scenario id. By passing\n",
    "    # np.newaxis we ensure that the shape is the same as the data\n",
    "    # next we take the absolute value\n",
    "    #\n",
    "    # basically we take the difference of the maximum across \n",
    "    # the row and the actual values in the row\n",
    "    #\n",
    "    outcome_regret = (data.max(axis=1).values[:, np.newaxis] - data).abs()\n",
    "    \n",
    "    overall_regret[outcome.name] = outcome_regret\n",
    "    max_regret[outcome.name] = outcome_regret.max()\n",
    "    "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_regret = pd.DataFrame(max_regret)\n",
    "sns.heatmap(max_regret/max_regret.max(), cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "colors = sns.color_palette()\n",
    "\n",
    "data = max_regret\n",
    "\n",
    "# makes it easier to identify the policy associated with each line\n",
    "# in the parcoords plot\n",
    "# data['policy'] = data.index.astype(\"float64\")\n",
    "\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "for i, (index, row) in enumerate(data.iterrows()):\n",
    "    paraxes.plot(row.to_frame().T, label=str(index), color=colors[i])\n",
    "paraxes.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "we clearly see a tradeoff between robustsness on maximum pollution and utility versus inertia and reliability. There is no compromise solution.\n",
    "\n",
    "Note that we have been looking at the maximum regret. I also saved the distribution of regret over the set of scenarios. So let's visualize this and see what we can learn from it\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "policy_regret = defaultdict(dict)\n",
    "for key, value in overall_regret.items():\n",
    "    for policy in value:\n",
    "        policy_regret[policy][key] = value[policy]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# this generates a 2 plots with a shared y and x axis\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,5), \n",
    "                         sharey=True, sharex=True)\n",
    "\n",
    "# to ensure easy iteration over the axes grid, we turn it\n",
    "# into a list. Because there are four plots, I hard coded\n",
    "# this. \n",
    "\n",
    "\n",
    "# zip allows us to zip together the list of axes and the list of \n",
    "# key value pairs return by items. If we iterate over this\n",
    "# it returns a tuple of length 2. The first item is the ax\n",
    "# the second items is the key value pair.\n",
    "for ax, (policy, regret) in zip(axes, policy_regret.items()):\n",
    "    data = pd.DataFrame(regret)\n",
    "\n",
    "    # we need to scale the regret to ensure fair visual\n",
    "    # comparison. We can do that by divding by the maximum regret\n",
    "    data = data/max_regret.max(axis=0)\n",
    "    sns.boxplot(data=data, ax=ax)\n",
    "    \n",
    "    # removes top and left hand black outline of axes\n",
    "    sns.despine()\n",
    "    \n",
    "    # ensure we know which policy the figure is for\n",
    "    ax.set_title(str(policy))\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is in line with the maximum regret parallel coordinates plot, but we get some more details."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We now have an understanding of which solutions have decent robustness using 2 different robustness metrics. \n",
    "\n",
    "A related but different question is to assess the uncertain conditions under which we get poor performance. For this, we can use scenario discovery. Since we want to identify the uncertainties only, we can remove the policy and lever columns from the experiments DataFrame. \n",
    "\n",
    "**Perform Scenario Discovery, focussed on understanding the conditions under which utility is lower than 0.35**\n",
    "\n",
    "note that it might happen that no scenario exists where this is the case. If so, increase the threshold to say 0.5 instead.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench.analysis import prim\n",
    "\n",
    "x = experiments.drop(columns=['policy', 'c1','c2', 'r1', 'r2', 'w1'])\n",
    "y = outcomes['utility'] < 0.5\n",
    "\n",
    "prim_alg = prim.Prim(x, y, threshold=0.5)\n",
    "box = prim_alg.find_box()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "box.inspect_tradeoff()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "the choice for box 42 is somewhat arbitrary. "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "box.inspect(42)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We see here that low utility is driven by delta. In practice, the next step is to ask why this happens. What is it about this delta parameter that explains the low performance on utility. Once this is clear, the next step is to ask what can be done about the situation to resolve this poor performance in case of a low delta parameter."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
